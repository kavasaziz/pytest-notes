#+title: pytest
#+startup: show2levels

* Pytest Testing Framework
** Helpers :noexport:
#+BEGIN_SRC emacs-lisp :session venv
(pyvenv-activate "~/venvs/frameworkenv/venv")
#+END_SRC
** Table of Contents :TOC:
- [[#pytest-testing-framework][Pytest Testing Framework]]
  - [[#installation][Installation]]
  - [[#pytest-introduction][PyTest Introduction]]
  - [[#test-searching][Test Searching]]
  - [[#save-time-and-code-with-test-fixtures][Save Time and Code with Test Fixtures]]
  - [[#basics-of-test-results-reporting-and-tracking-test-history][Basics of Test Results Reporting and Tracking Test History]]
  - [[#customizing-test-runs-with-the-command-line-and-configuration-files][Customizing Test Runs with the Command Line and Configuration Files]]
  - [[#handling-skips-and-expected-failures][Handling Skips and Expected Failures]]
  - [[#cross-browser-and-data-driven-testing-with-parametrize][Cross-Browser and Data-driven testing with parametrize]]
  - [[#fast-testing-with-pytest-xdist-and-parallel-vs-concurrent][Fast Testing with Pytest-xdist, and Parallel vs Concurrent]]
  - [[#writing-unit-tests-white-box-testing-with-tox][Writing Unit Tests (White Box Testing) with Tox]]
  - [[#writing-functional-tests-blackgrey-box-testing][Writing Functional Tests (Black/Grey Box Testing)]]

** Installation
Brief instructions on how to install the packages required for this repository on a Debian-based system are given below:

1. Update your package manager and install virtual environment for python.
  #+begin_src shell :shebang #!/usr/bin/env bash :results output
sudo apt-get update
sudo apt-get install python3-venv
  #+end_src

2. Upgrade the pip.
  #+begin_src shell :shebang #!/usr/bin/env bash :results output
python3 -m pip install --upgrade pip
  #+end_src

3. I will place my repository inside ~${HOME}/venvs/frameworkenv/~ folder. Therefore, create virtual environment inside this file.
  #+begin_src shell :shebang #!/usr/bin/env bash :results output
cd ~/venvs/frameworkenv/
python -m venv ./venv
  #+end_src

4. Activate virtual environment and install ~pytest~.
  #+begin_src shell :shebang #!/usr/bin/env bash :results output :dir ~/venvs/frameworkenv/
source ./bin/activate
python -m pip install pytest
  #+end_src

5. To quit from virtaul env just use ~deactivate~ command.
  #+begin_src shell :shebang #!/usr/bin/env bash :results output :dir ~/venvs/frameworkenv/
deactivate
  #+end_src

6. (Extra) To see the list of installed packages, you can use ~pip list~ or ~pip freeze~.
  #+begin_src shell :shebang #!/usr/bin/env bash :results output :dir ~/venvs/frameworkenv/
pip list 2>&1
pip freeze # This can be used to generate requirements.txt file also
  #+end_src

If you are facing another issue about management of python packges see more about python package management guide [[https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/][here]].
** PyTest Introduction
Test searching patterns of pytest is defined under =pytest.ini= file. The default patters are given below:
#+name: sample-[[pytest]]-ini
#+begin_src toml
[pytest]
python_files = test_*
python_classes = *Tests
python_functions = test_*
#+end_src

From those we can say that
- ~test_widget.py~ called files will be selected
- ~WidgetTests~ called class will be selected
- ~test_widget_click~ called function will be selected

To run tests just use ~pytest~ (add ~-v~ to have verbose output)
#+begin_src shell :shebang #!/usr/bin/env bash :results output
pytest -v
#+end_src

** Test Searching
Most common issue about test suites is that they require significant code refactoring when you want to create a new test suites, such as regression suite, smoke suit and unittest suite. This is caused since test file and test functions placement of tests are made for the suite. Therefore, for each suit, you will need to make a lot of copy paste, and which will make your suite and code hard to maintain.

Besides, it is not also easy to find location of the test under such a big code stack. Therefore, devide your test files and functions under more related units. For instance, if you see a failing test called ~test_ui_open~ you can easily that, it should be inside =entertainment= folder.
#+begin_example
.
├── pytest.ini
├── venv
└── tests
    └── sportscar
        ├── body
        ├── engine
        └── entertainment
#+end_example

Lets create quick fake tests files
#+begin_src shell :shebang #!/usr/bin/env bash :dir ~/venvs/frameworkenv/tests/sportscar :results output
for file in $(ls); do touch $file/test_$file.py ; done
#+end_src

and some fake tests:
- body
  #+begin_src python
def test_body_functions_as_expected():
    assert True
  #+end_src
- engine
  #+begin_src python
def test_engine_functions_as_expected():
    assert True
  #+end_src
- entertainment
  #+begin_src python
def test_entertainment_functions_as_expected():
    assert True
  #+end_src

Then lets re-run pytest
#+begin_src shell :shebang #!/usr/bin/env bash :dir ~/venvs/frameworkenv/ :results output
pytest -v
#+end_src

Now, we are able to run all the tests within a short amount of time. However, this is not the case always. Indeed, most of the time full test run of a test harnesses takes hours, even days. Therefore, as a developer, it will be improtant for you to run the test only related with the module that you make your development. For this purpose, we can use markers from ~pytest~. See below example


- body
  #+begin_src python
from pytest import mark


@mark.body
def test_body_functions_as_expected():
    assert True


@mark.smoke
@mark.body
def test_fundamental_body_functions_as_expected():
    assert True
  #+end_src
- engine
  #+begin_src python
from pytest import mark


@mark.engine
def test_engine_functions_as_expected():
    assert True


@mark.smoke
@mark.engine
def test_fundamental_engine_functions_as_expected():
    assert True
  #+end_src
- entertainment
  #+begin_src python
from pytest import mark


@mark.entertainment
def test_entertainment_functions_as_expected():
    assert True
  #+end_src

Then lets re-run pytest
#+begin_src shell :shebang #!/usr/bin/env bash :dir ~/venvs/frameworkenv/ :results output
pytest -v
#+end_src


Then lets re-run pytest only for =engine= mark
#+begin_src shell :shebang #!/usr/bin/env bash :dir ~/venvs/frameworkenv/ :results output
pytest -m engine -v
#+end_src

Notice that this will also generate a warning as given below:
#+begin_example
...
tests/sportscar/engine/test_engine.py:6: PytestUnknownMarkWarning: Unknown pytest.mark.engine - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @mark.engine
...
#+end_example

This is caused since we haven't register this marker explicitly. To avoid this warning, lets add following lines to =pytest.ini= file (see [[https://docs.pytest.org/en/stable/how-to/mark.html][details]]). When you re-run ~pytest -m engine -v~, you will notice that warning is disappeared.
#+name: pytest-ini-with-markers
#+begin_src toml
markers =
    engine: marks tests related with engine (select with '-m "engine"')
    body: marks tests related with body (select with '-m "body"')
    entertainment: marks tests related with entertainment (select with '-m "entertainment"')
    smoke: marks tests related with fundamental funcitonalities (select with '-m "smoke"')
#+end_src


I believe you also notice that body and engine tests are marked with smoke, so that we can run all the smoke test via ~pytest -m smoke~ . Even further, we can just run the smoke test only related with engine
#+begin_src shell :shebang #!/usr/bin/env bash :dir ~/venvs/frameworkenv/ :results output
pytest -m "engine and smoke" -v
#+end_src

- *OR*
  - ~pytest -m "not entertainment" -v~, run all the tests other than the entertainment suite
  - ~pytest -m "engine or smoke" -v~, run all the tests either related with engine or smoke suite
  - ~pytest -m "smoke and not body" -v~, run all the tests both related with smoke suite, but not inside body suite
  - ~pytest -m "not engine or smoke" -v~, un all the tests either not inside engine suite or within smoke suite.

As you can see, with the markers we are able to generate ou own custom suites just with simple parameters. So, pytest provides us simple and self maintaining system for suites. If you want to remove one of the test from smoke suite you can just remove the marker.

However, it has started to be hard to mark every single function with a marker. To overcome this, we can just mark the class and put the tests inside it. Lets do it with creating a new suite called battery suite.

Add following line to ~pytest.ini~ file
#+begin_src toml
    battery: marks tests related with battery (select with '-m "battery"')
#+end_src

and create following test class
#+begin_src python
from pytest import mark


@mark.battery
class BatteryTests:
    @mark.smoke
    def test_fundamental_battery_functions_as_expected(self):
        assert True


    def test_battery_functions_as_expected(self):
        assert True

#+end_src

Now, lets test our new suite
#+begin_src shell :shebang #!/usr/bin/env bash :dir ~/venvs/frameworkenv/ :results output
pytest -m "battery or smoke" -v
#+end_src

One final point is that, to list all the markers use ~pytest --markers~ (see ~pytest -h~ for further details)

** Save Time and Code with Test Fixtures

Okay lets say that we need to read the serial number of a car from a certain file before each test, and ensure that it is in a certain format. A sample file is given below:
#+begin_src text :tangle ~/venvs/frameworkenv/sample_file.txt
Some text SerialNumber=123456789
#+end_src

To parse this, we can use ~re~ package. However, writing those statements for each test will create a significant amout of code duplication, and maintanance cost.
#+begin_src python :results output
import re

with open('sample_file.txt', 'r') as file:
    sample_string = file.read()

# Define the regex pattern to match MAC addresses
pattern = r"SerialNumber=[0-9]+"

# Find all matches of the pattern in the string
matches = re.findall(pattern, sample_string)

# Extract MAC addresses from matches
serial_number = [match.split('=')[1] for match in matches]
print(serail_number[0])
#+end_src

To aviod this problem, we can use ~fixtures~, and ~fixtures~ are just a function used to avoid code duplicaitions. To make our fixture available to all the test cases, we should create it under =conftest.py=, which stands for /configure test/. So that, any fixture that we create in =conftest.py= becomes accessible anywhere inside that directory and any directory below it, any test case that is in that directory or any test case below it. Therefore, I will create =conftest.py= file under =tests= directory.

#+begin_src python
from pytest import fixture
import re


# function is the default scope
@fixture(scope='module')
def serial_number_from_file():
    with open('sample_file.txt', 'r') as file:
        sample_string = file.read()

    # Define the regex pattern to match MAC addresses
    pattern = r"SerialNumber=[0-9]+"

    # Find all matches of the pattern in the string
    matches = re.findall(pattern, sample_string)

    # Extract MAC addresses from matches
    serial_number = [match.split('=')[1] for match in matches]
    return serial_number

#+end_src

Here is a one important point to be aware of return value of fixtures have scopes, and there are 3 different scope option:
- ~function~: Returned object will be available through the functions scope and for multiple calls fixture will not be evaluated again. This also means that if I modify this value with the first usage inside the function, this change will be available in the remainin of function (Not much different from a variable).
- ~module~: Returned object will be available through the file scope and for multiple calls fixture will not be evaluated again throughout the test file. This also means that if I modify this value in first body test, this change will be available to body tests coming after it.
- ~session~: Returned object will be available through the test run and for multiple calls fixture will not be evaluated again throughout the test session. This also means that if I modify this value in body test, this change will be available to tests run after it.

Lets update file contents as below:
- =pytest.ini=
  #+begin_src toml :tangle ~/venvs/frameworkenv/pytest.ini
[pytest]
python_files = test_*
python_classes = *Tests
python_functions = test_*

markers =
    engine: marks tests related with engine (select with '-m "engine"')
    body: marks tests related with body (select with '-m "body"')
    entertainment: marks tests related with entertainment (select with '-m "entertainment"')
    smoke: marks tests related with fundamental funcitonalities (select with '-m "smoke"')
    battery: marks tests related with battery (select with '-m "battery"')
  #+end_src
- body
  #+begin_src python
from pytest import mark


@mark.body
def test_body_functions_as_expected():
    assert True


@mark.smoke
@mark.body
def test_fundamental_body_functions_as_expected(serial_number_from_file):
    print()
    serial_number = serial_number_from_file
    print("initial value", serial_number)
    serial_number.append("hello")
    print("value after append", serial_number)
    assert True
  #+end_src
- engine
  #+begin_src python
from pytest import mark


@mark.engine
def test_engine_functions_as_expected():
    assert True


@mark.smoke
@mark.engine
def test_fundamental_engine_functions_as_expected(serial_number_from_file):
    print()
    serial_number = serial_number_from_file
    print("initial value", serial_number)
    serial_number.append("hello")
    print("value after append", serial_number)
    assert True
  #+end_src
- entertainment
  #+begin_src python :tangle ~/venvs/frameworkenv/tests/sportscar/entertainment/test_entertainment.py
from pytest import mark


@mark.entertainment
def test_entertainment_functions_as_expected(serial_number_from_file):
    print()
    serial_number = serial_number_from_file
    print("initial value", serial_number)
    serial_number.append("hello")
    print("value after append", serial_number)
    assert True
  #+end_src
- battery
  #+begin_src python
from pytest import mark


@mark.battery
class BatteryTests:
    @mark.smoke
    def test_fundamental_battery_functions_as_expected(self, serial_number_from_file):
        print()
        serial_number = serial_number_from_file
        print("initial value", serial_number)
        serial_number.append("hello")
        print("value after append", serial_number)
        assert True

    @mark.smoke
    def test_other_fundamental_battery_functions_as_expected(self, serial_number_from_file):
        print()
        serial_number = serial_number_from_file
        print("initial value", serial_number)
        serial_number.append("hello")
        print("value after append", serial_number)
        assert True

    def test_battery_functions_as_expected(self):
        assert True
  #+end_src

Since the current scope is limitted to function calls. You will see that each function that is using the fixture will repeat following pattern:
#+begin_example
initial value ['123456789']
value after append ['123456789', 'hello']
#+end_example

Lets run the tests, in this point *do not forget to add ~-s~ option to see the print logs*:
#+begin_src shell :shebang #!/usr/bin/env bash :dir ~/venvs/frameworkenv/ :results output
pytest -m "smoke" -vs
#+end_src

To see the changes and differences for yourself, try different fixture scopes.


There is still one final point about the fixtures, which can also help you to understand the fixture scopes better. First update fixture as given below, and re-run the tests

#+begin_src python
from pytest import fixture
import re


# function is the default scope
@fixture(scope='module')
def serial_number_from_file():
    with open('sample_file.txt', 'r') as file:
        sample_string = file.read()

    # Define the regex pattern to match MAC addresses
    pattern = r"SerialNumber=[0-9]+"

    # Find all matches of the pattern in the string
    matches = re.findall(pattern, sample_string)

    # Extract MAC addresses from matches
    serial_number = [match.split('=')[1] for match in matches]
    print("\nFixture creted")
    yield serial_number
    print("\nFixture destroyed")
#+end_src

Sample output is given below:
#+begin_example
$ pytest -m "smoke" -vs
...
collected 8 items / 4 deselected / 4 selected

tests/sportscar/battery/test_battery.py::BatteryTests::test_fundamental_battery_functions_as_expected
Fixture creted

initial value ['123456789']
value after append ['123456789', 'hello']
PASSED
tests/sportscar/battery/test_battery.py::BatteryTests::test_other_fundamental_battery_functions_as_expected
initial value ['123456789', 'hello']
value after append ['123456789', 'hello', 'hello']
PASSED
Fixture destroyed

tests/sportscar/body/test_body.py::test_fundamental_body_functions_as_expected
Fixture creted

initial value ['123456789']
value after append ['123456789', 'hello']
PASSED
Fixture destroyed

tests/sportscar/engine/test_engine.py::test_fundamental_engine_functions_as_expected
Fixture creted

initial value ['123456789']
value after append ['123456789', 'hello']
PASSED
Fixture destroyed


========================================== 4 passed, 4 deselected in 0.02s ==========================================
#+end_example

** Basics of Test Results Reporting and Tracking Test History
One of the best way to represent the test results is to use html/xml.

In that point, it is good to point out again that while organising your tests please use ~tests/<module>/<class>::<function>~ structure while organising your tests. So that, you will notice how descriptive test names can be.

*** HTML
To achieve such test output in pytest, one can use ~pytest-html~ library. To install it use ~python -m pip install pytest-html~, and to grasp the results you can run pytest as given below:
#+begin_src shell :shebang #!/usr/bin/env bash :dir ~/venvs/frameworkenv/ :results output
pytest -v --html="results.html"
#+end_src

*Be careful that, if you use ~-s~ option to see the prints. Pytest will not be able to catch any test logs.*

This can also be useful to add some customization on this report, such as
- test suite/harness version, and current commit hash
- exporting the test results in other test output formats
- Format of html can be improved


*** XML
This output formating already comes with ~pytest~ by deafult. This is also one of the most commonly used test output formating, thanks to ~Junit~ framework. Lets run and see:
#+begin_src shell :shebang #!/usr/bin/env bash :dir ~/venvs/frameworkenv/ :results output
pytest -v --junitxml="results.xml"
#+end_src

/The good side of xml output is that it can be used to keep track of test history in jenkins./

** Customizing Test Runs with the Command Line and Configuration Files

We already discussed about how we can customize our test using markers. For instance, to run all the fundamental functionality test related with diesel sports car, we can use sth like below:
#+begin_src shell :shebang #!/usr/bin/env bash :dir ~/venvs/frameworkenv/ :results output
pytest -m "smoke and not battery" -v
#+end_src

However, as you could also notice, this is still not accurate enough because working mechanism of engine in electrical cars are much different from a diesel cars. To solve this, we can use marking ambigiously or we can use some fixtures. However, all those will add to much technical complexity over the usage of test suite. What we need is sth more like a recipe.

Therefore, what we need is dynamically adopting config mechanism. Even though it is not a good practice, we can use a class and embed our config generation and config values inside of it. (*In real world, config values should be kept inside a specific config file like ~config.json~.*)

#+begin_src python
class Config:
    SUPPORTED_ENGINE_TYPES = ["electrical", "diesel", "hybrit"]

    def __init__(self, engine_type):
        if engine_type.lower() not in self.SUPPORTED_ENGINE_TYPES:
            raise ValueError(f"{engine_type} is not amoung supported engine types: {self.SUPPORTED_ENGINE_TYPES}")

        self.engine = {
            "electrical": "Electrical Engine",
            "diesel": "Diesel Engine",
            "hybrid": "Hybrid Engine",
        }[engine_type]

#+end_src

In usual case, when we need to parse command line arguments, we use ~argparse~ package. However, ~pytest~ has on its own parsing system. Therefore, I will directly use it. However, note that this ~action~ argument and others still require the knowledge of [[https://docs.python.org/3/library/argparse.html][~argparse~ module]]. (see [[https://docs.python.org/3/library/argparse.html#quick-links-for-add-argument][action list]])

Finally, you can think ~parser~ as fixture provided via pytest itself in function given below:

#+begin_src python
def pytest_addoption(parser):
    parser.addoption(
        "--engine-type",
        action="store",
        # default="diesel",
        # dest='engine_type',
        help="Engine type for the vehicle under test"
    )
#+end_src

Even better, now you can see your custom option in pytest help output.

#+begin_src shell :shebang #!/usr/bin/env bash :dir ~/venvs/frameworkenv/ :results output
pytest --help | grep engine-type --context 5
#+end_src

See the example given below:
#+begin_example
                        additional metadata from a json string.
  --metadata-from-json-file=METADATA_FROM_JSON_FILE
                        additional metadata from a json file.

Custom options:
  --engine-type=ENGINE_TYPE
                        Engine type for the vehicle under test

[pytest] ini-options in the first pytest.ini|tox.ini|setup.cfg|pyproject.toml file found:

  markers (linelist):   Markers for test functions
#+end_example

Lets add getter for this option:
#+begin_src python

@fixture(scope='session')
def get_engine_type(request):
    return request.config.getoption("--engine-type")
#+end_src

Since this will not change throughout the test run, there is no reason to parse it again. Another point is that ~request~ parameter. One can again think of it as fixture provided via pytest.

Now update engine test, and see the results:
#+begin_src python
from pytest import mark


@mark.engine
def test_engine_functions_as_expected():
    assert True


@mark.smoke
@mark.engine
def test_fundamental_engine_functions_as_expected(serial_number_from_file, get_engine_type):
    print()
    engine_type = get_engine_type.lower()
    print(f"Engine type is {engine_type}")
    serial_number = serial_number_from_file
    print("initial value", serial_number)
    if ("electrical" == engine_type):
        serial_number.append("hello")
        print("value after append", serial_number)

    assert True
#+end_src

Then lets re-run pytest, and see the effect of =Electrical= engine over test run
#+begin_src shell :shebang #!/usr/bin/env bash :dir ~/venvs/frameworkenv/ :results output
pytest -m "smoke and engine" --engine-type=diesel -vs
#+end_src

Test output
#+begin_example
...
tests/sportscar/engine/test_engine.py::test_fundamental_engine_functions_as_expected
Engine type is diesel
initial value ['123456789']
PASSED
...
#+end_example

A better approach here would be to create an appropriate engine instance, and pass it to ~test_fundamental_engine_functions_as_expected~ test. It is not a good practice to variate the test content depending on certain parameters, and embedding instance specific test logic inside the test. In other words, we should not keep the logic that does not belong the test into test functions.

Therefore, first lets update our config class to include our engine logic
#+begin_src python
class Engine:
    SUPPORTED_ENGINE_TYPES = ["electrical", "diesel", "hybrid"]


    def __init__(self, engine_type):
        if engine_type is None:
            raise ValueError("engine_type can't be None")

        self.engine_type = engine_type.lower()

        if self.engine_type not in self.SUPPORTED_ENGINE_TYPES:
            raise ValueError(f"{engine_type} is not amoung supported engine types: {self.SUPPORTED_ENGINE_TYPES}")

        self.engine = {
            "electrical": "Electrical Engine",
            "diesel": "Diesel Engine",
            "hybrid": "Hybrid Engine",
        }[self.engine_type]



    def engine_run_fundamental_functions(self, serial_numbers):
        print(f"Engine type is {self.engine_type}")
        print("initial value", serial_numbers)
        if ("electrical" == self.engine_type):
            serial_numbers.append("hello")
            print("value after append", serial_numbers)

        return True

#+end_src

Then, lets add new fixture to manage global engine object. By the way, yes fixtures can use other fixtures, and you can use this to generate engine instance.
#+begin_src python
import re
from pytest import fixture

from config import Engine


# function is the default scope
@fixture(scope='module')
def serial_number_from_file():
    with open('sample_file.txt', 'r') as file:
        sample_string = file.read()

    # Define the regex pattern to match MAC addresses
    pattern = r"SerialNumber=[0-9]+"

    # Find all matches of the pattern in the string
    matches = re.findall(pattern, sample_string)

    # Extract MAC addresses from matches
    serial_number = [match.split('=')[1] for match in matches]
    return serial_number


def pytest_addoption(parser):
    parser.addoption(
        "--engine-type",
        action="store",
        # default="diesel", # not suggested for real-life practices
        # dest='engine_type',
        help="Engine type for the vehicle under test"
    )


@fixture(scope='session')
def get_engine_type(request):
    return request.config.getoption("--engine-type")


@fixture(scope='session')
def get_engine(get_engine_type):
    engine = Engine(get_engine_type)
    print(f"\n{engine.engine} created")
    yield engine
    print(f"\n{engine.engine} destroyed")
#+end_src

Now, lets update our test function
#+begin_src python
from pytest import mark


@mark.engine
def test_engine_functions_as_expected():
    assert True


@mark.smoke
@mark.engine
def test_fundamental_engine_functions_as_expected(serial_number_from_file, get_engine):
    assert True == get_engine.engine_run_fundamental_functions(serial_number_from_file)
#+end_src

Lets run and ensure that our changes are working:
#+begin_src shell :shebang #!/usr/bin/env bash :dir ~/venvs/frameworkenv/ :results output
pytest -m "smoke and engine" --engine-type=electrical -vs
# pytest -m "smoke and engine" --engine-type=diesel -vs
#+end_src

Here is the result of test run:
#+begin_example
...
tests/sportscar/engine/test_engine.py::test_fundamental_engine_functions_as_expected
Electrical Engine created
Engine type is electrical
initial value ['123456789']
value after append ['123456789', 'hello']
PASSED
Electrical Engine destroyed
...
#+end_example

** Handling Skips and Expected Failures

In a real-life scenario, it is expected to have failing test and skipped test. To deal with those, ~pytest~ provide us some special marks, like ~skip~ and ~xfail~. (see [[https://docs.pytest.org/en/latest/skipping.html][details]])

There are various approach for skipping or failing a test. However, it is better to remove the tests that will not expected to fixed in a sort term because your production suite should only have thing that working and bring valuable information. Why?
- failing and skipped test does not bring any value to anyone
- as the number of skip and fails increased amount of time for you to explain other why this does not pass increases
- as the fail keep existing, confidence to the result of test suite, and thus, the usage of test suite decrases.

Therefore, if you have test the which you don't expect to pass in short term, move it to some other branch. One other bad habit is to commenting the failing test, and this is a really bad practice because then you will never see the fail and you will never know it is existing. However, there will be a lot of stuffs that will be never tested. In other words, you are giving bad and wrong information to users.

However, if you are going to fix the fails within a short amount of time (at most within 2 sprints). It is okay to skip them.

Now, how we can do it? Lets say our test cases for testing the type of engine:
#+begin_src python
@mark.engine
def test_engine_type_diesel(get_engine_type):
    assert "diesel" == get_engine_type


@mark.engine
def test_engine_type_electrical(get_engine_type):
    assert "electrical" == get_engine_type
#+end_src

It is obvious that they will fail when we pass engine type as hybrid.
#+begin_src shell :shebang #!/usr/bin/env bash :dir ~/venvs/frameworkenv/ :results output
pytest -m "engine" --engine-type=hybrid -v
#+end_src

In order not to fail in those tests, lets sign them with skip. Thus, test run of the test suite will not fail any longer. Besides, it is also a good practice add notes about skip reason.
#+begin_src python
@mark.skip(reason="Broken after release train #")
@mark.engine
def test_engine_type_diesel(get_engine_type):
    assert "diesel" == get_engine_type


@mark.skip(reason="Not a compatible test case")
@mark.engine
def test_engine_type_electrical(get_engine_type):
    assert "electrical" == get_engine_type
#+end_src

To see the skip reason of the tests, add ~-rs~ flags together to the end of ~pytest~ command, as shown below:
#+begin_src shell :shebang #!/usr/bin/env bash :dir ~/venvs/frameworkenv/ :results output
pytest -m "engine" --engine-type=hybrid -v -rxs
#+end_src

Note that ~s~ inside the ~-rs~ command is not the one used to the print options. If you wish to see your prints inside the code please also add another ~-s~ option, as shown below:
#+begin_src shell :shebang #!/usr/bin/env bash :dir ~/venvs/frameworkenv/ :results output
pytest -m "engine" --engine-type=hybrid -v -rxs -s
#+end_src

As mentioned previously, it is also possible to use ~xfail~ for failing test cases, as shown below:
#+begin_src python
@mark.xfail(reason="Broken after release train #")
@mark.engine
def test_engine_type_diesel(get_engine_type):
    print("\nxfail test is executed")
    assert "diesel" == get_engine_type


@mark.skip(reason="Not a compatible test case")
@mark.engine
def test_engine_type_electrical(get_engine_type):
    print("\nskip test is not executed")
    assert "electrical" == get_engine_type
#+end_src

Just like in the ~skip~ case to see the fail reason of ~xfail~ tests, you need to add ~-rx~. In ~pytest~ docs, it is explained as below:
#+begin_src shell
pytest -rxXs  # show extra info on xfailed, xpassed, and skipped tests
#+end_src

Namely,
- ~-r~: show extra info
- ~-x~: on ~xfail~
- ~-X~: on ~xpass~
- ~-s~: on ~skip~

There is a lot about those markers, but for the sake of simplicity, difference between ~xfail~ and ~skip~ can be summarized as below:
- ~skip~: does not run the test if the condition is meet
- ~xfail~: does run the test and expects test to fail, and if it fails, it will give us xfail report if the contion above is meet. Otherwise it makes the test run fail. (Note: I know that ~xfail~ has argument for not to run the test, but this is not our subject for now)

The mentioned diffrence can also be seen via following command:
#+begin_src shell :shebang #!/usr/bin/env bash :dir ~/venvs/frameworkenv/ :results output
pytest -m "engine" --engine-type=hybrid -v -rxs -s
#+end_src

Inside the output of above command, one should expect to see only the ="xfail test is executed"= statement, and not the one inside skipped tests.

Different from the ~skip~, there are some possible usa cases for ~xfail~, and one of them is testing the negative test cases.

It is common to test failing test cases so that we can make sure that case expected to be fail is failing. For instance, you have a deprecated feature in your app, and thus you expect it not to be supported. Therefore, you can expect to have failing test for such a case. Lets add a sample test for it:
#+begin_src python
@mark.xfail(reason="This future should be deprecated after release trian #")
@mark.engine
def test_engine_deprecated_future(get_engine_type):
    assert False
#+end_src

/Finally, as we have discussed previously, it is not a good practice to leave failing tests aroud for a long time. In our currect case, they are only used for demo purposes. Therefore, I will leave them as they are for the future commits./

We would also like to point out that we have given a brief introduction to how these signs can be used. We will explore more detailed use cases in the future.

** Cross-Browser and Data-driven testing with parametrize

First of all, lets uncomment ~default~ option for parser, as shown below:
#+begin_src python
def pytest_addoption(parser):
    parser.addoption(
        "--engine-type",
        action="store",
        default="diesel", # not suggested for real-life practices
        # dest='engine_type',
        help="Engine type for the vehicle under test"
    )
#+end_src

/Please notice that not using ~--engine-type~ flag will not make our test runs fail as of this point./

I know this is not a good practice, but I don't want to add extra cummulative complexities over our test runs. Besides, it is not related with the subject that we will mention any longer. Therefore, it is okay to leave it as default for now. Therefore, the final version of the lastly updated files will be as given below:
- =conftest.py=
  #+begin_src python
import re
from pytest import fixture

from config import Engine


# function is the default scope
@fixture(scope='module')
def serial_number_from_file():
    with open('sample_file.txt', 'r') as file:
        sample_string = file.read()

    # Define the regex pattern to match MAC addresses
    pattern = r"SerialNumber=[0-9]+"

    # Find all matches of the pattern in the string
    matches = re.findall(pattern, sample_string)

    # Extract MAC addresses from matches
    serial_number = [match.split('=')[1] for match in matches]
    return serial_number


def pytest_addoption(parser):
    parser.addoption(
        "--engine-type",
        action="store",
        default="diesel", # not suggested for real-life practices
        # dest='engine_type',
        help="Engine type for the vehicle under test"
    )


@fixture(scope='session')
def get_engine_type(request):
    return request.config.getoption("--engine-type")


@fixture(scope='session')
def get_engine(get_engine_type):
    engine = Engine(get_engine_type)
    print(f"\n{engine.engine} created")
    yield engine
    print(f"\n{engine.engine} destroyed")
  #+end_src

- =test_engine.py=
  #+begin_src python :tangle ~/venvs/frameworkenv/tests/sportscar/engine/test_engine.py
from pytest import mark


@mark.engine
def test_engine_functions_as_expected():
    assert True


@mark.smoke
@mark.engine
def test_fundamental_engine_functions_as_expected(serial_number_from_file, get_engine):
    assert True == get_engine.engine_run_fundamental_functions(serial_number_from_file)


@mark.xfail(reason="Broken after release train #")
@mark.engine
def test_engine_type_diesel(get_engine_type):
    print("\nxfail test is executed")
    assert "diesel" == get_engine_type


@mark.skip(reason="Not a compatible test case")
@mark.engine
def test_engine_type_electrical(get_engine_type):
    print("\nskip test is not executed")
    assert "electrical" == get_engine_type


@mark.xfail(reason="This future should be deprecated after release trian #")
@mark.engine
def test_engine_deprecated_future(get_engine_type):
    assert False
  #+end_src

Parametrization is one of the most important point to have modular and scalable test suite. Basically, one can think parametrization as running the same test with different inputs and program states without code duplication. Even though there are more than 20 different ways have parametrized tests, we will discuss about 3 effective and beginner friendly ways to have parametrized tests in our test suite. (see [[https://docs.pytest.org/en/7.0.x/example/parametrize.html#paramexamples][docs1]], [[https://docs.pytest.org/en/7.0.x/how-to/parametrize.html#parametrize-basics][docs2]])

*** On-the-fly parametrization /(NOT RECOMMENDED)/
Parametrization in ~pytest~ is also handled via ~mark~ decorator. Let's say, we need our body to appropriate for every type of engine. Therefore, produced device body needs to tested for every single engine type. Further, there are multiple types of structure for car body. Besides, since such a test would be a fundamental requirment for both body and engine, it needs to be added in to both smoke, body and engine suites.

#+begin_src python
@mark.smoke
@mark.body
@mark.engine
@mark.parametrize(
    "body_type", [("coupes"), ("cabriolets")]
)
def test_body_type_compatible_with_engine(body_type, get_engine_type):
    print(f"\n{body_type} is compatible with engine {get_engine_type}")
    assert True
#+end_src


Lets run and see the output:
#+begin_src shell :shebang #!/usr/bin/env bash :dir ~/venvs/frameworkenv/ :results output
pytest -m "engine and smoke and body" -v -rxs -s
# pytest -m "engine and smoke and body" --engine-type=hybrid -v -rxs -s
#+end_src

In this point, note that the value of parametirazed arguments need to be list of tuples, because we can have more than one input arguments. For instance, we can give set input states for the test:
#+begin_src python
@mark.smoke
@mark.body
@mark.engine
@mark.parametrize(
    "body_type, engine_type, expected", [
        ("coupes", "electrical", True),
        ("cabriolets", "electrical", True),
        ("coupes", "hybrid", False),
        ("cabriolets", "hybrid", True),
        ("coupes", "diesel", True),
        ("cabriolets", "diesel", True),
    ]
)
def test_body_engine_compatibility(body_type, engine_type, expected):
    print(f"\n{body_type} is compatible with engine {engine_type}")
    if body_type == "coupes" and engine_type == "hybrid":
        assert False == expected
    else:
        assert True == expected

#+end_src

Now, lets run and see how the parametrization of multiple variables will work.
#+begin_src shell :shebang #!/usr/bin/env bash :dir ~/venvs/frameworkenv/ :results output
pytest -m "engine and smoke and body" -v -rxs -s
#+end_src

Note that, in above example we tested the device for some fake engine type with hardcoded parameter. Further, when we are trying to avoid from code duplication, we have ended up with a lot of manually written test states. Even though it is practical to have such parametrization for not easily producible test states, it was not necessary in our example. I will refoctor this approach, but before that lets first look at how we could decrase that repaated input states. Since, even if we decouple ~expected~ parameter from the test parametrization, we still have to list each ~engine_type~ for each ~body_type~.

To solve it, one can just add a new ~@mark.parametrize~ for other variable seperately.
#+begin_src python
@mark.smoke
@mark.body
@mark.engine
@mark.parametrize(
    "body_type", [
        ("coupes"),
        ("cabriolets"),
    ]
)
@mark.parametrize(
    "engine_type", [
        ("electrical"),
        ("hybrid"),
        ("diesel"),
    ]
)
def test_body_engine_compatibility(body_type, engine_type):
    print(f"\n{body_type} is compatible with engine {engine_type}")
    assert True

#+end_src

Now, lets run and see that number of executed tests will not change.
#+begin_src shell :shebang #!/usr/bin/env bash :dir ~/venvs/frameworkenv/ :results output
pytest -m "engine and smoke and body" -v -rxs -s
#+end_src

However, all those are still coupling the input data with indvidual test case. In other words, this will make our test suite hard to scale, maintaince and refactor. For instance, if it is needed to add new body type, we need add it to every single place where we have made this parametrization.

*** Indirect parametrization with fixtures
To solve this issue, we can apply parametization over fixtures, so that we can have parametrized input management over single point. Lets add a fixture called ~get_body_type~,

#+begin_src python
from config import Body


@fixture(scope="session", params=["coupes", "cabriolets"])
def get_body_list(request):
    body_type = request.param
    yield body_type


@fixture(scope="module")
def get_body(get_body_list):
    body_type = get_body_list
    body_instance = Body(body_type)
    yield body_instance
#+end_src

So, any test that uses this fixture will be executed for each element inside =params= list. Now, we have a mothod to generate body types; however, this still does not allow us to decouple the test logic from the existing structure:

#+begin_src python
class Body:
    SUPPORTED_BODY_TYPES = ["coupes", "cabriolets"]


    def __init__(self, body_type):
        if body_type is None:
            raise ValueError("body_type can't be None")

        self.body_type = body_type.lower()

        if self.body_type not in self.SUPPORTED_BODY_TYPES:
            raise ValueError(f"{body_type} is not amoung supported body types: {self.SUPPORTED_BODY_TYPES}")

        self.body = {
            "coupes": "coupes body",
            "cabriolets": "cabriolets body",
        }[self.body_type]

        self.supported_engines = {
            "coupes": ["electrical", "diesel"],
            "cabriolets": ["electrical", "diesel", "hybrid"],
        }[self.body_type]


    def is_engine_compatible(self, engine):
        if engine.engine_type in self.supported_engines:
            return True
        return False
#+end_src

Since we have updated our structure, now we can rewrite our test.
#+begin_src python
@mark.smoke
@mark.body
@mark.engine
def test_body_engine_compatibility(get_body, get_engine):
    is_compatible = get_body.is_engine_compatible(get_engine)
    if is_compatible:
        print(f"\n{get_body.body_type} is compatible with engine {get_engine.engine_type}")
    else:
        print(f"\n{get_body.body_type} is not compatible with engine {get_engine.engine_type}")

    assert is_compatible

#+end_src

Now, lets run and see our test results:
#+begin_src shell :shebang #!/usr/bin/env bash :dir ~/venvs/frameworkenv/ :results output
pytest -m "engine and smoke and body" -v -rxs -s
#+end_src

As you also observed, since the default type of engine was ~diesel~ our tests has passed. However, if we would run the tests with ~--engine-type=hybrid~ flag, the test will fail for ~coupes~ body type bacause it is not among the one of supported engine types for ~coupes~.

#+begin_src shell :shebang #!/usr/bin/env bash :dir ~/venvs/frameworkenv/ :results output
pytest -m "engine and smoke and body" --engine-type=hybrid -v -rxs -s
#+end_src

#+begin_quote
 In this point, if the ~hybrid~ engine is not supported on purpose, it will be a good practice to sign this test case with ~xfail~. Thus, the nagative test case will be tested.
#+end_quote

By refactoring the test below we can handle expected fail
#+begin_src python
from pytest import xfail


@mark.smoke
@mark.body
@mark.engine
def test_body_engine_compatibility(get_body, get_engine):
    is_compatible = get_body.is_engine_compatible(get_engine)
    if is_compatible:
        print(f"\n{get_body.body_type} is compatible with engine {get_engine.engine_type}")
    else:
        print(f"\n{get_body.body_type} is not compatible with engine {get_engine.engine_type}")

    if not is_compatible and get_body.body_type == 'coupes' and get_engine.engine_type == 'hybrid':
        xfail(reason=f"{get_engine.engine_type} engine is not supported for {get_body.body_type}")
    assert is_compatible

#+end_src

Now, lets run and see the result for our ~xfail~.
#+begin_src shell :shebang #!/usr/bin/env bash :dir ~/venvs/frameworkenv/ :results output
pytest -m "engine and smoke and body" --engine-type=hybrid -v -rxs -s
#+end_src

This solution is satisfactory for the current test scenario. However, for handling compatibility information across multiple test cases, it is recommended to either transform this compatibility test into a fixture or establish a dependency tree within test modules. Alternatively, a combination of both approaches can be utilized for enhanced management of compatibility data. Since, those are out of our subject, they will revisited in the future.

*** Data-Driven Parametrization

Until now, we have achieved to decouple our tests from the test data and test logic. However, there is still some room to improve. Currently, our test data resides within the test suite, which could become unwieldy with large datasets. Therefore, a more structured approach involves managing test data via dedicated data files, such as JSON or YAML. To illustrate, we'll utilize a files named =config.json= and =test_data/test_data.json= in this part.

Test data is unique for specifci use case, and thus, there will be multiple of them. Therefore, unlike the config file, it is not intiutive to read it once and keep it into memory throughout the session because same test data will not be used multiple times.

Until now, we have used test data for test parametrization. Therefore, ~body_type~ can be put inside test data file.
#+begin_src json
{
    "body_types": ["coupes", "cabriolets"]
}
#+end_src

As for config file, it should include general information about test execution and test modules. For instance, ~supported_engines~ can be used for this purpose.
#+begin_src json :tangle ~/venvs/frameworkenv/config.json
{
    "supported_engines": {
        "coupes": ["electrical", "diesel"],
        "cabriolets": ["electrical", "diesel", "hybrid"]
    }
}
#+end_src

To utilize test data and config file, we need to update parsing  mechnism used for them.
#+begin_src python
import json

from config import Body


CONFIG_PATH="config.json"


@fixture(scope="session")
def get_config(path=CONFIG_PATH):
    with open(path) as config_file:
        yield json.load(config_file)


TEST_DATA_PATH="test_data/test_data.json"


def load_test_data(path=TEST_DATA_PATH):
    with open(path) as test_data_file:
        return json.load(test_data_file)


@fixture(scope="session", params=load_test_data()["body_types"])
def get_body_list(request):
    body_type = request.param
    yield body_type


@fixture(scope="session")
def get_body(get_body_list, get_config):
    body_type = get_body_list
    body_instance = Body(body_type, get_config["supported_engines"])
    yield body_instance
#+end_src

Finally, it is also necessary to update =config.py= because it still does not use supported engine types provided via =config.json=.
#+begin_src python
class Body:
    SUPPORTED_BODY_TYPES = ["coupes", "cabriolets"]


    def __init__(self, body_type, supported_engines):
        if body_type is None:
            raise ValueError("body_type can't be None")

        self.body_type = body_type.lower()

        if self.body_type not in self.SUPPORTED_BODY_TYPES:
            raise ValueError(f"{body_type} is not amoung supported body types: {self.SUPPORTED_BODY_TYPES}")

        self.body = {
            "coupes": "coupes body",
            "cabriolets": "cabriolets body",
        }[self.body_type]

        self.supported_engines = supported_engines[self.body_type]


    def is_engine_compatible(self, engine):
        if engine.engine_type in self.supported_engines:
            return True
        return False
#+end_src

Now, lets run and make sure that we are able to fetch required information properly from config and test data file.
#+begin_src shell :shebang #!/usr/bin/env bash :dir ~/venvs/frameworkenv/ :results output
pytest -m "engine and smoke and body" --engine-type=hybrid -v -rxs -s
#+end_src

In this point, note that turning the path of =config.json= and =test_data/test_data.json= into a command line argument would be a better practice. For now, I will update cli arguments for =config.json=, but for test data this is rather complicated and beyond our knowledge for now. Therefore, I will leave it as future work. Together with this change, the final version of updated files are given below:

- =conftest.py=
  #+begin_src python
import re
import json

from pytest import fixture

from config import Body
from config import Engine


# function is the default scope
@fixture(scope='module')
def serial_number_from_file():
    with open('sample_file.txt', 'r') as file:
        sample_string = file.read()

    # Define the regex pattern to match MAC addresses
    pattern = r"SerialNumber=[0-9]+"

    # Find all matches of the pattern in the string
    matches = re.findall(pattern, sample_string)

    # Extract MAC addresses from matches
    serial_number = [match.split('=')[1] for match in matches]
    return serial_number


# Add parsers for cli arguments
def pytest_addoption(parser):
    parser.addoption(
        "--engine-type",
        action="store",
        default="diesel", # not suggested for real-life practices
        # dest='engine_type',
        help="Engine type for the vehicle under test"
    )
    parser.addoption(
        "--config",
        action="store",
        default="config.json", # not suggested
        help="Path of config file"
    )


# cli argument return options
@fixture(scope='session')
def get_engine_type(request):
    return request.config.getoption("--engine-type")


@fixture(scope='session')
def get_config_path(request):
    return request.config.getoption("--config")


TEST_DATA_PATH = "test_data/test_data.json"


def load_test_data(path=TEST_DATA_PATH):
    with open(path) as test_data_file:
        return json.load(test_data_file)


# Test instances used throughout the test execution
@fixture(scope='session')
def get_engine(get_engine_type):
    engine = Engine(get_engine_type)
    print(f"\n{engine.engine} created")
    yield engine
    print(f"\n{engine.engine} destroyed")


@fixture(scope="session")
def get_config(get_config_path):
    with open(get_config_path) as config_file:
        yield json.load(config_file)


@fixture(scope="session", params=load_test_data()["body_types"])
def get_body_list(request):
    body_type = request.param
    yield body_type


@fixture(scope="module")
def get_body(get_body_list, get_config):
    body_type = get_body_list
    body_instance = Body(body_type, get_config["supported_engines"])
    yield body_instance
  #+end_src

- =config.py=
  #+begin_src python :tangle ~/venvs/frameworkenv/tests/config.py
class Engine:
    SUPPORTED_ENGINE_TYPES = ["electrical", "diesel", "hybrid"]


    def __init__(self, engine_type):
        if engine_type is None:
            raise ValueError("engine_type can't be None")

        self.engine_type = engine_type.lower()

        if self.engine_type not in self.SUPPORTED_ENGINE_TYPES:
            raise ValueError(f"{engine_type} is not amoung supported engine types: {self.SUPPORTED_ENGINE_TYPES}")

        self.engine = {
            "electrical": "Electrical Engine",
            "diesel": "Diesel Engine",
            "hybrid": "Hybrid Engine",
        }[self.engine_type]



    def engine_run_fundamental_functions(self, serial_numbers):
        print(f"Engine type is {self.engine_type}")
        print("initial value", serial_numbers)
        if ("electrical" == self.engine_type):
            serial_numbers.append("hello")
            print("value after append", serial_numbers)

        return True


class Body:
    SUPPORTED_BODY_TYPES = ["coupes", "cabriolets"]


    def __init__(self, body_type, supported_engines):
        if body_type is None:
            raise ValueError("body_type can't be None")

        self.body_type = body_type.lower()

        if self.body_type not in self.SUPPORTED_BODY_TYPES:
            raise ValueError(f"{body_type} is not amoung supported body types: {self.SUPPORTED_BODY_TYPES}")

        self.body = {
            "coupes": "coupes body",
            "cabriolets": "cabriolets body",
        }[self.body_type]

        self.supported_engines = supported_engines[self.body_type]


    def is_engine_compatible(self, engine):
        if engine.engine_type in self.supported_engines:
            return True
        return False
  #+end_src

- =test_body.py=
  #+begin_src python :tangle ~/venvs/frameworkenv/tests/sportscar/body/test_body.py
from pytest import mark
from pytest import xfail


@mark.body
def test_body_functions_as_expected():
    assert True


@mark.smoke
@mark.body
def test_fundamental_body_functions_as_expected(serial_number_from_file):
    print()
    serial_number = serial_number_from_file
    print("initial value", serial_number)
    serial_number.append("hello")
    print("value after append", serial_number)
    assert True


@mark.smoke
@mark.body
@mark.engine
def test_body_engine_compatibility(get_body, get_engine):
    is_compatible = get_body.is_engine_compatible(get_engine)
    if is_compatible:
        print(f"\n{get_body.body_type} is compatible with engine {get_engine.engine_type}")
    else:
        print(f"\n{get_body.body_type} is not compatible with engine {get_engine.engine_type}")

    if not is_compatible and get_body.body_type == 'coupes' and get_engine.engine_type == 'hybrid':
        xfail(reason=f"{get_engine.engine_type} engine is not supported for {get_body.body_type}")
    assert is_compatible
  #+end_src

** Fast Testing with Pytest-xdist, and Parallel vs Concurrent
To add parallel execution capability to ~pytest~, we use ~pyest-xdist~ package. This can help us to gain significant amount of time while we are working on a IO bounded works.
#+begin_src shell :shebang #!/usr/bin/env bash :results output
python -m pip install pytest-xdist
#+end_src

Then run the ~pytest~ with ~-n<number-of-threads>~ flag introduced via ~pytest-xdist~
#+begin_src shell :shebang #!/usr/bin/env bash :dir ~/venvs/frameworkenv/ :results output
pytest -m "battery" -v -rxs -s -n4
#+end_src

One issue with paralel testing if you haven't write isolated tests, namely one of the test is effecting the state of other test, such as growing serial number list, errors that you have faced may not be reporducible. Besides, this is not only the case for parallel executions, it is also true even if the tests are not run in parallel. Therefore, to utilize parallel execution capability and to have less error-pruno test suite, write isolated tests case. If you wish to create a complex test state, use test data files.

#+begin_quote
 Depending on number passed to ~-n~ flag, ~pytest-xdist~ can decide on running a package the test suite parallel or concurrently. To leave the decision to system, you can just pass ~-nauto~
#+end_quote

For our scenario, we can easily say battery units inside a car does not necessarily needs to be tested in sequence. Besides, testing batteries can take longer than you can think of. For this purpose first lets add some other test data to our suite:

#+begin_src json :tangle ~/venvs/frameworkenv/test_data/test_data.json
{
    "body_types": ["coupes", "cabriolets"],
    "battery_units": [
        {"type_info": "alpha", "capacity": "5"},
        {"type_info": "gamma", "capacity": "5"},
        {"type_info": "prime", "capacity": "5"}
    ]
}
#+end_src

However, in this point, there are some refactorization issues that we need to consider:
- Parser options has started get large and hard to read. Therefore, we need to make it easy to read and refactor
- To avoid code duplicaitions, we need to parameterize battery intances via fixture, just as we did previously.

Considering all the issues mentioned above the final version of =conftest.py= is given below:
#+begin_src python :tangle ~/venvs/frameworkenv/tests/conftest.py
import re
import json

from pytest import fixture

from config import Body
from config import Engine
from config import Battery

# function is the default scope
@fixture(scope='module')
def serial_number_from_file():
    with open('sample_file.txt', 'r') as file:
        sample_string = file.read()

    # Define the regex pattern to match MAC addresses
    pattern = r"SerialNumber=[0-9]+"

    # Find all matches of the pattern in the string
    matches = re.findall(pattern, sample_string)

    # Extract MAC addresses from matches
    serial_number = [match.split('=')[1] for match in matches]
    return serial_number


# Add parsers for cli arguments
def pytest_addoption(parser):
    # default values not suggested for real-life practices
    options = [
        ("--engine-type", "diesel", "Engine type for the vehicle under test"),
        ("--config", "config.json", "Path of config file"),
    ]

    for option, default, help_msg in options:
        parser.addoption(
            option,
            action="store",
            default=default,
            help=help_msg
        )


# cli argument return options
@fixture(scope='session')
def get_engine_type(request):
    return request.config.getoption("--engine-type")


@fixture(scope='session')
def get_config_path(request):
    return request.config.getoption("--config")


TEST_DATA_PATH = "test_data/test_data.json"


def load_test_data(path=TEST_DATA_PATH):
    with open(path) as test_data_file:
        return json.load(test_data_file)


# Test instances used throughout the test execution
@fixture(scope='session')
def get_engine(get_engine_type):
    engine = Engine(get_engine_type)
    print(f"\n{engine.engine} created")
    yield engine
    print(f"\n{engine.engine} destroyed")


@fixture(scope="session")
def get_config(get_config_path):
    with open(get_config_path) as config_file:
        yield json.load(config_file)


@fixture(scope="session", params=load_test_data()["body_types"])
def get_body_list(request):
    body_type = request.param
    yield body_type


@fixture(scope="module")
def get_body(get_body_list, get_config):
    body_type = get_body_list
    body_instance = Body(body_type, get_config["supported_engines"])
    yield body_instance


@fixture(scope="session", params=load_test_data()["battery_units"])
def get_battery_info(request):
    battery_unit = request.param
    yield battery_unit


@fixture(scope="module")
def get_battery_unit(get_battery_info):
    battery_info = get_battery_info
    battery_unit = Battery(battery_info)
    yield battery_unit
#+end_src

Another point of testing is to keep our test logic seperated from executed code. For instance, if we are going to test the battery health measurments, the code making measurment should be seperated from the one which will test the measurments. Therefore, we need to add another class here to =config.py= to encapsulte our battery logic.
#+begin_src python :tangle ~/venvs/frameworkenv/tests/config.py
import time

class Battery:

    SUPPORTED_BATTERY_TYPES = ["alpha", "gamma", "prime"]


    def __init__(self, battery_info):
        if battery_info["type_info"] is None:
            raise ValueError("body_type can't be None")

        self.type_info = battery_info["type_info"].lower()

        if self.type_info not in self.SUPPORTED_BATTERY_TYPES:
            raise ValueError(f"{self.type_info} is not amoung supported body types: {self.SUPPORTED_BATTERY_TYPES}")

        self.battery_interface = {
            "alpha": "INTERFACE_A",
            "gamma": "INTERFACE_G",
            "prime": "INTERFACE_P"
        }[self.type_info]

        self.capacity = int(battery_info["capacity"])


    def battery_measure_cell_health(self):
        time.sleep(self.capacity)
        return True
#+end_src

In that point, health measurment of our battery will take time proportional with the capacity of battery, so that we can see the effect of parallel execution clearly.

Finally, we need to rewrite our test for battery:
#+begin_src python :tangle ~/venvs/frameworkenv/tests/sportscar/battery/test_battery.py
from pytest import mark


@mark.battery
class BatteryTests:
    @mark.smoke
    def test_fundamental_battery_functions_as_expected(self, serial_number_from_file):
        print()
        serial_number = serial_number_from_file
        print("initial value", serial_number)
        serial_number.append("hello")
        print("value after append", serial_number)
        assert True

    @mark.smoke
    def test_other_fundamental_battery_functions_as_expected(self, serial_number_from_file):
        print()
        serial_number = serial_number_from_file
        print("initial value", serial_number)
        serial_number.append("hello")
        print("value after append", serial_number)
        assert True

    @mark.smoke
    def test_battery_units_health(self, get_battery_unit):
        assert True == get_battery_unit.battery_measure_cell_health()


    def test_battery_functions_as_expected(self):
        assert True
#+end_src

Now, run the battery test and see how long does it take:
#+begin_src shell :shebang #!/usr/bin/env bash :dir ~/venvs/frameworkenv/ :results output
pytest -m "battery" -v -rxXs -s
#+end_src

As you could also notice, it took more than 15s as expected. However, by allowing parallel test execution, we can decreate it upto 5-6s.
#+begin_src shell :shebang #!/usr/bin/env bash :dir ~/venvs/frameworkenv/ :results output :session venv
time pytest -m "battery" -v -rxXs -s -n auto
#+end_src

Here is the sample output of it, and you can also see consumed time below:
#+begin_example
...
plugins: html-4.1.1, xdist-3.5.0, metadata-3.1.1
created: 8/8 workersinitialized: 1/8 workersinitialized: 2/8 workersinitialized: 3/8 workersinitialized: 4/8 workersinitialized: 5/8 workersinitialized: 6/8 workersinitialized: 7/8 workersinitialized: 8/8 workersready: 1/8 workers      collecting: 1/8 workerscollecting: 1/8 workerscollecting: 2/8 workerscollecting: 2/8 workerscollecting: 2/8 workerscollecting: 3/8 workerscollecting: 4/8 workerscollecting: 4/8 workerscollecting: 4/8 workerscollecting: 5/8 workerscollecting: 6/8 workerscollecting: 6/8 workerscollecting: 6/8 workerscollecting: 7/8 workers8 workers [6 items]
scheduling tests via LoadScheduling

tests/sportscar/battery/test_battery.py::BatteryTests::test_battery_functions_as_expected
tests/sportscar/battery/test_battery.py::BatteryTests::test_battery_units_health[get_battery_info2]
tests/sportscar/battery/test_battery.py::BatteryTests::test_battery_units_health[get_battery_info1]
tests/sportscar/battery/test_battery.py::BatteryTests::test_battery_units_health[get_battery_info0]
tests/sportscar/battery/test_battery.py::BatteryTests::test_other_fundamental_battery_functions_as_expected
tests/sportscar/battery/test_battery.py::BatteryTests::test_fundamental_battery_functions_as_expected
[gw0] PASSED tests/sportscar/battery/test_battery.py::BatteryTests::test_fundamental_battery_functions_as_expected
[gw6] PASSED tests/sportscar/battery/test_battery.py::BatteryTests::test_battery_functions_as_expected
[gw1] PASSED tests/sportscar/battery/test_battery.py::BatteryTests::test_other_fundamental_battery_functions_as_expected
[gw3] PASSED tests/sportscar/battery/test_battery.py::BatteryTests::test_battery_units_health[get_battery_info1]
[gw2] PASSED tests/sportscar/battery/test_battery.py::BatteryTests::test_battery_units_health[get_battery_info0]
[gw4] PASSED tests/sportscar/battery/test_battery.py::BatteryTests::test_battery_units_health[get_battery_info2]

...

real	0m5.706s
user	0m2.561s
sys	0m0.199s
#+end_example

** Writing Unit Tests (White Box Testing) with Tox
** Writing Functional Tests (Black/Grey Box Testing)
